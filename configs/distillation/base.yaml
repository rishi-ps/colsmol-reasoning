# MaxSim Interaction Distillation (MID) - Base Config
# Idea 1: Visual Token Distillation

teacher:
  model_name: "vidore/colpali-v1.3"
  device: "cpu"  # Keep teacher on CPU to save VRAM

student:
  model_name: "vidore/colSmol-256M"
  use_lora: true
  lora_rank: 32
  lora_alpha: 32
  device: "cuda"

training:
  # Loss weights
  alpha: 1.0   # Contrastive
  beta: 1.0    # Interaction (KL)
  gamma: 0.5   # Ranking
  
  learning_rate: 2.0e-5
  batch_size: 2
  gradient_accumulation_steps: 8  # Effective batch size = 16
  num_epochs: 1
  warmup_steps: 100

output:
  checkpoint_dir: "checkpoints/distillation"
